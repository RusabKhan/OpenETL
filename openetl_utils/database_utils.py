"""
This module contains utility functions for working with SQLAlchemy engine connections.

Class:
- SQLAlchemyEngine: Represents a class to connect with any database using SQLAlchemy.

Methods:
- __init__: Initializes the class with database connection details.
- test: Tests the connection to the database.
- get_metadata: Retrieves schema metadata from the connection.
- execute_query: Executes a SQL query against the connection.
- get_metadata_df: Retrieves schema metadata in a dataframe format.
"""
import calendar
import os
import sys
from typing import List, Type

import sqlalchemy as sq
import pandas as pd

from openetl_utils.__migrations__.app import OpenETLDocument, OpenETLOAuthToken
from openetl_utils.__migrations__.batch import OpenETLBatch
from openetl_utils.__migrations__.scheduler import OpenETLIntegrations, OpenETLIntegrationsRuntimes
from sqlalchemy import MetaData, Table, Column, and_, select, PrimaryKeyConstraint, func, text, inspect, or_
from sqlalchemy.orm import sessionmaker

from openetl_utils.cache import sqlalchemy_database_engines
from openetl_utils.enums import ConnectionType
from sqlalchemy.exc import OperationalError, NoResultFound
from openetl_utils.enums import ColumnActions
import numpy as np
from sqlalchemy.ext.declarative import declarative_base
from pyspark.sql.types import StringType, IntegerType, FloatType, BooleanType, TimestampType, ArrayType, MapType
from datetime import datetime, timedelta
from sqlalchemy.schema import CreateSchema
import logging
from croniter import croniter

class DatabaseUtils():
    
    """
    A class to connect with any database using SQLAlchemy.

    Attributes:
    - engine (string): Sqlalchemy dialect
    - hostname (string): Your database hostname
    - username (string): Your database username
    - password (string): Your database password
    - port (string): Your database port
    - database (string): Your database
    - connection_name (string, optional): Description of the connection. Defaults to None.
    - connection_type (string, optional): Description of the connection type. Defaults to None.

    Methods:
    - __init__: Initializes the class with database connection details.
    - test: Tests the connection to the database.
    - get_metadata: Retrieves schema metadata from the connection.
    - execute_query: Executes a SQL query against the connection.
    - get_metadata_df: Retrieves schema metadata in a dataframe format.
    - dataframe_details: Generates details about each column in a DataFrame.
    - create_table: Creates a new table in the database.
    - fill_na_based_on_dtype: Replaces NaN values in a DataFrame based on column data types.
    - alter_table_column_add_or_drop: Alters a table by adding or dropping a column.
    - drop_table: Drops the specified table from the database.
    - truncate_table: Truncates a table by deleting all rows.
    - cast_columns: Casts columns in a DataFrame to specific data types.
    - map_to_spark_type: Maps Pandas DataFrame data types to Spark DataFrame data types.
    - match_pandas_schema_to_spark: Matches data types of columns in a Spark DataFrame to a specified schema.
    - write_data: Writes data to a table in the database.
    - create_session: Creates a new session and initializes metadata and base.
    - commit_changes: Commits the changes made within the session.
    - close_session: Closes the session and sets the session close flag.
    - __enter__: Enters the 'with' context and creates a new session.
    - __exit__: Exits the 'with' context, commits changes, and closes the session.
    - __dispose__: Disposes the session and engine.
    - create_schema_if_not_exists: Creates a schema in the database if it does not already exist.
    - alter_table_column_add_primary_key: Alters a table by adding a primary key to a specified column.
    - create_document_table: Creates a document table in the database.
    - create_batch_table: Creates a batch table in the database.
    - fetch_rows: Executes a select query on the specified table with provided conditions.
    - fetch_document: Fetches a single document based on the specified table, schema, and conditions.
    - write_document: Writes a document to the specified table in the database.
    - get_created_connections: Returns a list of created connections for the specified connector type.
    - insert_openetl_batch: Inserts a new OpenETLBatch instance into the database.
    - update_openetl_batch: Updates an OpenETLBatch instance in the database.
    - get_dashboard_data: Retrieves dashboard data including total counts and integration details.
    """

    _connections = {}  # Class-level dictionary to store connections

    def __init__(self, engine=None, hostname=None, username=None, password=None, port=None, database=None,
                 connection_name=None, connection_type=None):
        """
                 Initialize a DatabaseUtils instance with the specified database connection parameters.
                 
                 If an engine is provided, this method generates a unique connection key based on the engine, hostname, port, database, and username. It then checks the class-level _connections dictionary for an existing engine corresponding to this key. If found, the existing engine is reused; otherwise, a new SQLAlchemy engine is created using a URL constructed from the provided details and the dialect mapping from sqlalchemy_database_engines, and it is stored in _connections. Finally, a SQLAlchemy session is created by calling self.create_session().
                 
                 If engine is None, the instanceâ€™s engine is set to None and session creation is skipped.
                 
                 Parameters:
                     engine (str, optional): Identifier for the SQLAlchemy database dialect (e.g., 'postgresql', 'mysql'). If None, no database connection will be established.
                     hostname (str, optional): The hostname of the database server.
                     username (str, optional): The username for authentication with the database.
                     password (str, optional): The password for authentication with the database.
                     port (str, optional): The port number on which the database server is listening.
                     database (str, optional): The name of the target database.
                     connection_name (str, optional): A custom name for the connection. Defaults to None.
                     connection_type (str, optional): A description of the connection type. Defaults to None.
                 """
        if engine is None:
            self.engine = None
            return

        self.connection_key = f"{engine}_{hostname}_{port}_{database}_{username}"

        if self.connection_key in DatabaseUtils._connections:
            self.engine = DatabaseUtils._connections[self.connection_key]
        else:
            engine_dialect = sqlalchemy_database_engines[engine]
            url = f"{engine_dialect}://{username}:{password}@{hostname}:{port}/{database}"
            self.engine = sq.create_engine(url=url)

            DatabaseUtils._connections[self.connection_key] = self.engine

        self.create_session()

    def test(self):
        """
        Tests the database connection by attempting to establish a connection using the configured SQLAlchemy engine.
        
        This method calls self.engine.connect() to verify that a connection to the database can be opened. If the connection attempt fails, an exception raised by the SQLAlchemy engine will propagate. On success, it returns True.
        
        Returns:
            bool: True if the connection was successfully established.
        """
        self.engine.connect()
        return True

    def get_metadata(self):
        """Get schema metadata from the connection

        Returns:
            dict: {"tables": tables,"schema":[]}
        """
        inspector = sq.inspect(self.engine)
        schemas = inspector.get_schema_names()
        tables = None

        for schema in schemas:
            print(f"schema: {schema}")
            tables = inspector.get_table_names(schema=schema)
        return {schema: tables}

    def execute_query(self, query):
        """Execute query against the connection

        Args:
            query (string): Valid SQL query

        Returns:
            Dataframe: Pandas dataframe of your query results
        """
        con = self.engine.connect()
        data = con.execute(text(query))
        return pd.DataFrame(data)

    def get_metadata_df(self):
        """Get your schema metadata in a dataframe

        Returns:
            Dataframe: Pandas dataframe of your schema
        """
        inspector = sq.inspect(self.engine)
        schemas = inspector.get_schema_names()
        tables = []
        data = {}
        for schema in schemas:
            data_schema = []
            print(f"schema: {schema}")
            tables = inspector.get_table_names(schema=schema)
            data["Table Name"] = tables
            while len(tables) < len(data_schema):
                data_schema.append(schema)
            data["Schema"] = schema
        return pd.DataFrame(data)

    # TO HANDLE DML TASKS

    def dataframe_details(self, df):
        """
        Generate a dictionary containing details about each column in the DataFrame.

        Parameters:
            df (DataFrame): The input DataFrame for which details are to be generated.

        Returns:
            dict: A dictionary where keys are column names and values are their data types.
        """
        details = {}
        for col in df.columns:
            dtype = df[col].dtype.name
            # Mapping Pandas data types to SQLAlchemy data types
            if dtype == 'float64':
                dtype = 'Float'
            elif dtype == 'int64':
                dtype = 'Integer'
            elif dtype == 'bool':
                dtype = 'Boolean'
            elif dtype == 'object':
                dtype = 'String'
            elif dtype == 'datetime64[ns]':
                dtype = 'DateTime'
            elif dtype == 'timedelta64[ns]':
                dtype = 'Interval'
            elif dtype == 'category':
                dtype = 'Enum'
            elif dtype == 'bytes':
                dtype = 'LargeBinary'
            elif dtype == 'unicode':
                dtype = 'UnicodeText'
            elif dtype == 'period':
                dtype = 'Interval'
            elif dtype == 'object':
                if df[col].apply(lambda x: isinstance(x, dict)).any():
                    dtype = 'Dictionary'
                elif df[col].apply(lambda x: isinstance(x, list)).any():
                    dtype = 'Array'
            else:
                dtype = 'String'
            details[col] = str(dtype)
        return details

    def create_table(self, table_name: str, df: pd.DataFrame, target_schema="public"):
        """
        Create a new table in the database. If already exists, skip creation.

        Args:
            table_name (str): The name of the table.
            schema_details (dict): column_name with python datatypes. Valid values are `str`, `float`, `bool`, `int`, `list`.
        Returns:
            tuple: A tuple indicating the success status and a message.
        """
        schema_details = self.dataframe_details(df)
        table = Table(table_name, self.metadata,
                      *[Column(column_name, eval(column_type)) for column_name, column_type in schema_details.items()], schema=target_schema)
        self.metadata.create_all(self.engine)

        self.schema_details = schema_details

        return True, table_name

    def fill_na_based_on_dtype(self, df):
        """
        Replace NaN values in a Pandas DataFrame based on the data type of each column.

        Parameters:
            df: Pandas DataFrame.

        Returns:
            DataFrame: DataFrame with NaN values replaced based on column data types.
        """
        nan_replacements = {
            'int64': 0,
            'float64': 0.0,
            'bool': False,
            'object': ' ',
            'string': ' ',
            'datetime64[ns]': pd.Timestamp('1970-01-01'),
            'timedelta64[ns]': pd.Timedelta('0 days'),
            'category': ' ',
            'bytes': b' ',
            'unicode': u' ',
            # Add more data types as needed
        }
        # Add more variations if needed
        nan_variations = ['nan', 'NaN', 'Nan', 'naN', 'NAN']

        for col in df.columns:
            dtype = df[col].dtype.name
            if dtype in nan_replacements:
                # Replace variations of NaN values
                df[col] = df[col].map(lambda x: nan_replacements[dtype] if str(
                    x).strip().lower() in nan_variations else x)

        return df

    def alter_table_column_add_or_drop(self, table_name, column_name=None, column_details=None, action: ColumnActions = ColumnActions.ADD):
        """
        Alters a SQLAlchemy table by either adding or dropping a column.

        Args:
            action:
            table_name (str): The name of the table to be altered.
            column_name (str): The name of the column to be added or dropped. Required if drop_column is False.
            column_details (str): The details of the column to be added. Required if drop_column is False.

        Returns:
            tuple: A tuple containing a boolean indicating success or failure and a message.

         """
        # Reflect the existing table from the database
        table = Table(table_name, self.metadata,
                      autoload_with=self.engine)

        if action == ColumnActions.DROP:
            table._columns.remove(table.c[column_name])
            action = f"Dropped column '{column_name}' from table '{table_name}'."

        elif action == ColumnActions.ADD:
            new_column = Column(column_name, column_details)
            table.append_column(new_column)
            action = f"Added column '{column_name}' to table '{table_name}'."

        elif action == ColumnActions.MODIFY:
            raise NotImplementedError

        # Save changes to the database
        self.metadata.drop_all(self.engine)
        self.metadata.create_all(self.engine)

        return True, action

    def drop_table(self, table_name: str):
        """
        Drop the specified table from the database.

        Args:
            table_name (str): The name of the table to drop.

        Returns:
            tuple: A tuple indicating the success status and a message.
        """
        self.metadata.reflect(bind=self.engine, only=[table_name])
        existing_table = self.metadata.tables[table_name]
        existing_table.drop(self.engine)

        return True, f"Table '{table_name}' has been dropped."

    def truncate_table(self, table_name):
        """
        Truncates a table by deleting all rows.

        Args:
            table_name (str): The name of the table to truncate.

        Returns:
            tuple: A tuple indicating the success status and a message.
                   The success status is True if truncation is successful, False otherwise.
                   The message provides information about the truncation result.
        """
        self.metadata.reflect(bind=self.engine, only=[table_name])
        table = self.metadata.tables[table_name]
        with self.engine.connect() as connection:
            delete_statement = table.delete()
            connection.execute(delete_statement)

        return True, f"Table '{table_name}' truncated."

    def cast_columns(self, df):
        """
        Function to cast columns in a DataFrame to specific data types based on the majority of data types in the columns.

        Parameters:
        - self: The object instance
        - df: The DataFrame containing the columns to be cast

        Returns:
        - df: The DataFrame with columns cast to specific data types
        """
        for col in df.columns:
            types_counts = {
                str: df[col].apply(lambda x: isinstance(x, str)).sum(),
                int: df[col].apply(lambda x: isinstance(x, int)).sum(),
                float: df[col].apply(lambda x: isinstance(x, float)).sum(),
                list: df[col].apply(lambda x: isinstance(x, list) or isinstance(x, np.ndarray)).sum(),
                dict: df[col].apply(lambda x: isinstance(x, dict)).sum(),
                bool: df[col].apply(lambda x: isinstance(x, bool)).sum(),
                np.datetime64: pd.api.types.is_datetime64_any_dtype(df[col]),
                np.timedelta64: pd.api.types.is_timedelta64_dtype(df[col]),
                bytes: df[col].apply(lambda x: isinstance(x, bytes)).sum()
            }
            majority_type = max(types_counts, key=types_counts.get)
            if majority_type == list or majority_type == np.ndarray:
                df[col] = df[col].apply(lambda x: list(
                    x) if isinstance(x, np.ndarray) else x).astype(str)
            elif majority_type == np.datetime64:
                df[col] = df[col].astype('datetime64[ns]')
            elif majority_type == np.timedelta64:
                df[col] = df[col].astype('timedelta64[ns]')
            else:
                df[col] = df[col].astype(majority_type)
        return df

    def map_to_spark_type(self, pandas_dtype):
        """
        Map Pandas DataFrame data types to equivalent Spark DataFrame data types.

        Parameters:
            pandas_dtype (str): Pandas DataFrame data type.

        Returns:
            Spark DataFrame data type.
        """
        spark_type_map = {
            'Object': StringType(),
            'Integer': IntegerType(),
            'Float': FloatType(),
            'Boolean': BooleanType(),
            'Datetime': TimestampType(),
            # Spark does not have a direct equivalent for timedelta
            'Interval': StringType(),
            'Enum': StringType(),  # Defaulting to StringType for Pandas categories
            'LargeBinary': StringType(),  # Defaulting to StringType for bytes
            'UnicodeText': StringType(),  # Defaulting to StringType for Unicode
            'Interval': StringType(),  # Defaulting to StringType for period
            # Mapping to ArrayType with StringType elements
            'Array': ArrayType(StringType()),
            # Mapping to MapType with StringType keys and values
            'Dictionary': MapType(StringType(), StringType()),
            # Add more mappings as needed
        }
        # Default to StringType for unrecognized types
        return spark_type_map.get(pandas_dtype, StringType())

    def match_pandas_schema_to_spark(self, spark_df, schema_details=None):
        """
        Match the data types of columns in a Spark DataFrame to a specified schema.

        Parameters:
            schema_details:
            spark_df (DataFrame): Spark DataFrame.

        Returns:
            DataFrame: Spark DataFrame with matched data types.
        """
        schema_details = schema_details if schema_details else self.schema_details
        for col, dtype in schema_details.items():
            spark_dtype = self.map_to_spark_type(dtype)
            spark_df = spark_df.withColumn(
                col, spark_df[col].cast(spark_dtype))
        return spark_df

    def write_data(self, data, table_name='openetl_batches', if_exists='append', schema="public"):
        """
        Writes data to a table in etl_batches.

        Parameters:
            df (DataFrame): The DataFrame to write to the table.
            table_name (str): The name of the table to write to.
        """

        df = pd.DataFrame(data, index=[0])
        print(df)
        with self.engine.connect() as con:
            df.to_sql(
                table_name, con=con, if_exists=if_exists, index=False, schema=schema)
        return True

    def create_session(self):
        """
        Create a new session and initialize metadata and base.
        """
        Session = sessionmaker(bind=self.engine)
        self.session = Session()
        self.Base = declarative_base()
        self.metadata = MetaData()

        self.is_session_close = False
        return True

    def commit_changes(self):
        """
        Commit the changes made within the session.
        """
        self.session.commit()

    def close_session(self):
        """
        Close the session and set the session close flag.
        """
        self.session.close()
        self.is_session_close = True

    # TO HANDLE `with` CONTEXT MANAGER
    def __enter__(self):
        """
        Enter the `with` context and create a new session.
        """
        self.create_session()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """
        Exit the `with` context and commit changes, close the session.
        """
        self.commit_changes()
        self.close_session()

    def __dispose__(self):
        """
        Dispose the session and engine.
        """
        logging.info("DISPOSING SCHEMA UTILS SQLALCHEMY ENGINE")
        self.engine.dispose()

    def create_schema_if_not_exists(self, schema_name='open_etl'):
        """
        Creates a schema in the database if it does not already exist.

        Parameters:
            schema_name (str): The name of the schema to create. Defaults to 'open_etl'.

        Returns:
            None
        """
        with self.engine.connect() as connection:
            connection.execute(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")

    def alter_table_column_add_primary_key(self, table_name, column_name='id', schema_name='open_etl'):
        """
        Alter a table by adding a primary key to a specified column.

        Args:
            table_name (str): The name of the table to alter.
            column_name (str): The name of the column to set as the primary key. Defaults to 'id'.
            Currently not working changes do not reflect in the database.

        Returns:
            bool: True if the primary key addition is successful, False otherwise.
        """

        existing_table = Table(
            table_name, self.metadata, autoload_with=self.engine, schema=schema_name)

        column = existing_table.columns[column_name]
        primary_key_constraint = PrimaryKeyConstraint(column)
        existing_table.append_constraint(primary_key_constraint)
        self.metadata.create_all(self.engine)

        return True



    def create_table_from_base(self, target_schema="public",base=None):
        try:
            if not self.engine.dialect.has_schema(self.engine, "public"):
                self.engine.execute(CreateSchema(target_schema))
        except Exception as e:
            # If schema already exists, it will raise a ProgrammingError which we can ignore
            pass
        metadata = MetaData(schema=target_schema)

        # Inspect the existing table structure to check for differences
        inspector = inspect(self.engine)
        table_exists = inspector.has_table(base.__tablename__, schema=target_schema)

        if table_exists:
            model_columns = {}
            for column_name, column in base.__table__.columns.items():
                model_columns[column_name] = column.type # Convert type to string

            # Get columns from the table in the database
            inspector = inspect(self.engine)
            existing_columns = [col['name'] for col in inspector.get_columns(base.__tablename__)]

            # Identify missing columns
            missing_columns = {col: col_type for col, col_type in model_columns.items() if col not in existing_columns}

            extra_columns = [col for col in existing_columns if col not in model_columns]

            # Drop extra columns from the table
            for column_name in extra_columns:
                self.alter_table_column_add_or_drop(table_name=base.__tablename__,
                                                    column_name=column_name,
                                                    column_details=None, action=ColumnActions.DROP)

            # Add missing columns to the table
            for column_name, column_type_sql in missing_columns.items():
                self.alter_table_column_add_or_drop(table_name=base.__tablename__,
                                                        column_name=column_name,
                                                    column_details=column_type_sql, action=ColumnActions.ADD)
        else:
            # Create the table if it doesn't exist
            base.metadata.create_all(self.engine)



    def fetch_rows(self, table_name='openetl_documents', schema_name='open_etl', conditions: dict = {}):
        """
        Executes a select query on the specified table with the provided conditions.

        Args:
            table_name (str, optional): The name of the table to fetch rows from. Defaults to 'openetl_documents'.
            schema_name (str, optional): The schema of the table. Defaults to 'open_etl'.
            conditions (dict, optional): The conditions to filter the rows. Defaults to {}.

        Returns:
            ResultProxy: The result of the select query.
        """

        table = Table(table_name, self.metadata,
                      autoload_with=self.engine, schema=schema_name)
        document_column = table.columns.document
        where_condition = []

        for key, value in conditions.items():
            where_condition.append(table.columns[key] == value)

        columns = [table.columns[column_name]
                   for column_name in table.columns.keys()]
        data = ''
        result = None
        # Execute a select query with the WHERE clause
        with self.engine.connect() as connection:
            query = select(columns).where(and_(*where_condition))
            result = connection.execute(query)
        return result

    def fetch_document(self, table_name='openetl_documents', schema_name='open_etl', conditions: dict = {}):
        """
        Fetches a single document based on the specified table, schema, and conditions.

        Args:
            table_name (str, optional): The name of the table to fetch the document from. Defaults to 'openetl_documents'.
            schema_name (str, optional): The schema of the table. Defaults to 'open_etl'.
            conditions (dict, optional): The conditions to filter the document retrieval. Defaults to {}.

        Returns:
            dict: A dictionary representing the fetched document with column names as keys.
        """

        result = self.fetch_rows(
            table_name=table_name, schema_name=schema_name, conditions=conditions)
        data = result.fetchall()
        columns = result.keys()
        rows_with_column_names = [
            {column_name: row_value for column_name, row_value in zip(columns, row)} for row in data]

        return rows_with_column_names[0]

    def write_document(self, document, table_name='openetl_documents', schema_name='open_etl') -> tuple[bool, str]:
        """
        Writes a document to the specified table in the database.

        Args:
            document (dict): The document to be written. It should contain a 'document' key with the document content.
            table_name (str, optional): The name of the table to write the document to. Defaults to 'openetl_documents'.
            schema_name (str, optional): The schema of the table. Defaults to 'open_etl'.

        Returns:
            bool: True if the document is successfully written, False otherwise.

        Raises:
            Exception: If an error occurs while writing the document. The error message is logged.
        """
        Session = sessionmaker(bind=self.engine)
        session = Session()

        # Create an instance of OpenETLDocument
        new_document = OpenETLDocument(
            connection_credentials=document['connection_credentials'],
            connection_name=document['connection_name'],
            connection_type=document['connection_type'],
            auth_type=document['auth_type'],
            connector_name=document['connector_name'],
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        session.add(new_document)
        session.commit()

        session.close()
        return True, ""


    def delete_document(self,  document_id: int=None):
        """
        Deletes a document from the specified table in the database.

        Args:
            document_id:

        Returns:
            bool: True if the document is successfully deleted, False otherwise.

        Raises:
            Exception: If an error occurs while deleting the document. The error message is logged.
        """
        Session = sessionmaker(bind=self.engine)
        session = Session()
        document = session.query(OpenETLDocument).filter_by(id=document_id).first()
        integrations = (
            session.query(OpenETLIntegrations)
            .filter(or_(
                OpenETLIntegrations.source_connection == document_id,
                OpenETLIntegrations.target_connection == document_id
            ))
            .all()
        )
        if integrations:
            for integration in integrations:
                session.delete(integration)
                session.commit()
        if document:
            session.delete(document)
            session.commit()
            session.close()
            return True, ""
        else:
            session.close()
            raise NoResultFound



    def get_created_connections(self, connector_type=None, connection_name=None, id=None) -> pd.DataFrame:
        """
        Returns a list of created connections for the specified connector type.

        Args:
            connector_type (ConnectionType): The value of type of connector, defaults to ConnectionType.DATABASE.

        Returns:
            list: A dataframe of created connections.
        """
        columns_to_fetch = [
            OpenETLDocument.connection_name,
            OpenETLDocument.connection_type,
            OpenETLDocument.auth_type,
            OpenETLDocument.connector_name,
            OpenETLDocument.connection_credentials,
            OpenETLDocument.id
        ]
        conditions = []

        if connector_type is not None:
            conditions.append(OpenETLDocument.connection_type == connector_type)
        if connection_name is not None:
            conditions.append(OpenETLDocument.connection_name == connection_name)
        if id is not None:
            conditions.append(OpenETLDocument.id == id)

        # Construct the query
        if conditions:
            select_query = select(*columns_to_fetch).where(and_(*conditions))
        else:
            select_query = select(*columns_to_fetch)

        # Execute the query and fetch data into a DataFrame
        data = pd.read_sql(select_query, self.session.bind)
        result = data.to_dict(orient='records')

        return result

    def save_oauth_token(self, access_token, refresh_token, expires_in, scope, connection_id):
        """
        Args:
            access_token:
            refresh_token:
            expires_in:
            scope:
            connection_id:
        """
        oauth_ = OpenETLOAuthToken(
            access_token=access_token,
            refresh_token=refresh_token,
            expires_in=expires_in,
            scope=scope,
            connection=connection_id
        )
        self.session.add(oauth_)
        self.session.commit()
        return True

    def get_oauth_token(self, connection_id):
        return self.session.query(OpenETLOAuthToken).filter(OpenETLOAuthToken.connection == connection_id).one_or_none()

    def delete_oauth_token(self, connection_id):
        self.session.query(OpenETLOAuthToken).filter(OpenETLOAuthToken.connection == connection_id).delete(synchronize_session=False)
        self.session.commit()
        return True


    def insert_openetl_batch(self, start_date, integration_id, batch_type, batch_status, batch_id, integration_name, rows_count=0, end_date=None):
        """
        Inserts a new OpenETLBatch instance into the database.

        Args:
            integration_id: The ID of the integration.
            start_date (datetime): The start date of the batch.
            batch_type (str): The type of the batch.
            batch_status (str): The status of the batch.
            batch_id (str): The unique identifier of the batch.
            integration_name (str): The name of the integration.
            rows_count (int, optional): The number of rows in the batch. Defaults to 0.
            end_date (datetime, optional): The end date of the batch. Defaults to None.

        Returns:
            OpenETLBatch: The newly created OpenETLBatch instance.
        """
        # Get the current highest batch_id
        session = self.session

        # Create new OpenETLBatch instance
        new_batch = OpenETLBatch(
            batch_id=batch_id,
            integration_id=integration_id,
            start_date=start_date,
            end_date=end_date,
            batch_type=batch_type,
            batch_status=batch_status,
            integration_name=integration_name,
            rows_count=rows_count
        )

        # Add and commit the new batch to the session
        session.add(new_batch)
        session.commit()
        return new_batch


    def update_openetl_batch(self, batch_id, integration_id, **kwargs):
        """
        Updates an OpenETLBatch object in the database with the specified batch_id.

        Args:
            batch_id (int): The ID of the batch to update.
            **kwargs: Keyword arguments specifying the fields to update and their new values.

        Returns:
            OpenETLBatch: The updated OpenETLBatch object.

        Raises:
            Exception: If no OpenETLBatch object with the specified batch_id is found.
        """
        
        session = self.session
        # Find the batch by batch_id
        batch = session.query(OpenETLBatch).filter(
            OpenETLBatch.batch_id == batch_id, OpenETLBatch.integration_id == integration_id).one_or_none()

        if batch is not None:
            for key, value in kwargs.items():
                if value is not None:
                    if key in ['rows_count']:
                        existing_value = getattr(batch, key)
                        value = existing_value + value
                    setattr(batch, key, value)

            session.commit()
            return batch
        else:
            raise NoResultFound


    def update_openetl_document(self, document_id, **kwargs):
        """
        Updates an OpenETLBatch object in the database with the specified batch_id.

        Args:
            document_id (int): The ID of the batch to update.
            **kwargs: Keyword arguments specifying the fields to update and their new values.

        Returns:
            OpenETLBatch: The updated OpenETLBatch object.

        Raises:
            Exception: If no OpenETLBatch object with the specified batch_id is found.
        """

        session = self.session
        # Find the batch by batch_id
        batch = session.query(OpenETLDocument).filter(
            OpenETLDocument.id == document_id).one_or_none()

        if batch is not None:
            # Update the specified fields
            for key, value in kwargs.items():
                if value is not None:
                    setattr(batch, key, value)
            session.commit()
            return batch
        else:
            raise NoResultFound

    def get_dashboard_data(self, page: int = 1, per_page: int = 30):
        """
        Retrieves dashboard data including total counts and paginated integration details.

        Args:
            page (int, optional): The page number for integrations. Defaults to 1.
            per_page (int, optional): The number of items per page for integrations. Defaults to 30.

        Returns:
            dict: A dictionary containing total counts and paginated integration details.
        """

        session = self.session

        # Retrieve total counts
        total_api_connections = session.query(OpenETLDocument).filter(
            OpenETLDocument.connection_type == ConnectionType.API.value).count()
        total_db_connections = session.query(OpenETLDocument).filter(
            OpenETLDocument.connection_type == ConnectionType.DATABASE.value).count()
        total_pipelines = session.query(OpenETLIntegrations).count()
        total_rows_migrated = session.query(
            func.sum(OpenETLIntegrationsRuntimes.row_count)).scalar()

        # Retrieve integration details with pagination
        offset = (page - 1) * per_page

        latest_runs_subquery = session.query(
            OpenETLIntegrationsRuntimes.integration,
            func.max(OpenETLIntegrationsRuntimes.start_date).label('latest_start_date')
        ).group_by(OpenETLIntegrationsRuntimes.integration).subquery()

        integrations_query = session.query(
            OpenETLIntegrationsRuntimes.integration,
            OpenETLIntegrationsRuntimes.run_status,
            OpenETLIntegrationsRuntimes.start_date,
            OpenETLIntegrationsRuntimes.end_date,
            OpenETLIntegrationsRuntimes.error_message
        ).join(
            latest_runs_subquery,
            (OpenETLIntegrationsRuntimes.integration == latest_runs_subquery.c.integration) &
            (OpenETLIntegrationsRuntimes.start_date == latest_runs_subquery.c.latest_start_date)
        ).order_by(
            OpenETLIntegrationsRuntimes.created_at.desc()
        ).offset(offset).limit(per_page)

        integrations = integrations_query.all()

        # Total integration count for pagination
        total_integrations = session.query(latest_runs_subquery.c.integration).count()

        # Retrieve run counts by integration
        run_counts = session.query(
            OpenETLIntegrationsRuntimes.integration,
            func.count(OpenETLIntegrationsRuntimes.id).label('run_count')
        ).group_by(OpenETLIntegrationsRuntimes.integration).all()

        run_count_dict = {run.integration: run.run_count for run in run_counts}

        integrations_dict = [
            {
                "integration_name": integration.integration,
                "latest_run_status": integration.run_status,
                "start_date": integration.start_date.isoformat() if integration.start_date else None,
                "end_date": integration.end_date.isoformat() if integration.end_date else None,
                "error_message": integration.error_message,
                "run_count": run_count_dict.get(integration.integration, 0)
            }
            for integration in integrations
        ] if integrations else []

        total_pages = (total_integrations + per_page - 1) // per_page

        # Return the dashboard data with pagination
        return {
            "page": page,
            "per_page": per_page,
            "total_items": total_integrations,
            "total_pages": total_pages,
            'total_api_connections': total_api_connections or 0,
            'total_db_connections': total_db_connections or 0,
            'total_pipelines': total_pipelines or 0,
            'total_rows_migrated': total_rows_migrated or 0,
            'integrations': {
                "data": integrations_dict
            }
        }

    def get_all_integration(self, page: int = 1, per_page: int = 30, integration_id=None):
        """
        Get all integrations paginated.

        Args:
            page (int, optional): The page number. Defaults to 1.
            per_page (int, optional): The number of items per page. Defaults to 30.

        Returns:
            dict: A dictionary containing the paginated results.
        """
        offset = (page - 1) * per_page
        schedulers = None
        total_items = 0

        if integration_id:
            schedulers = self.session.query(OpenETLIntegrations) \
                .filter(OpenETLIntegrations.id == integration_id) \
                .order_by(OpenETLIntegrations.created_at.desc())

            total_items = schedulers.count()
            schedulers = schedulers.offset(offset).limit(per_page).all()
        else:
            schedulers = self.session.query(OpenETLIntegrations) \
                .order_by(OpenETLIntegrations.created_at.desc()) \
                .offset(offset) \
                .limit(per_page)
            total_items = self.session.query(OpenETLIntegrations).count()
            schedulers = schedulers.all()

        results = [
            {
                "id": scheduler.id,
                "integration_name": scheduler.integration_name,
                "integration_type": scheduler.integration_type,
                "cron_expression": [parse_cron_expression(cron) for cron in scheduler.cron_expression],
                "is_running": scheduler.is_running,
                "is_enabled": scheduler.is_enabled,
                "created_at": scheduler.created_at.isoformat() if scheduler.created_at else None,
                "updated_at": scheduler.updated_at.isoformat() if scheduler.updated_at else None,
            }
            for scheduler in schedulers
        ]
        total_pages = (total_items + per_page - 1) // per_page

        return {
            "page": page,
            "per_page": per_page,
            "total_items": total_items,
            "total_pages": total_pages,
            "data": results
        }

    def get_integration_history(self, integration_id, page: int = 1, per_page: int = 30):

        offset = (page - 1) * per_page

        history = self.session.query(OpenETLIntegrationsRuntimes) \
            .filter(OpenETLIntegrationsRuntimes.integration == integration_id) \
            .order_by(OpenETLIntegrationsRuntimes.created_at.desc())

        total_items = history.count()
        total_pages = (total_items + per_page - 1) // per_page
        history = history \
            .offset(offset) \
            .limit(per_page).all()


        return {
            "page": page,
            "per_page": per_page,
            "total_items": total_items,
            "total_pages": total_pages,
            "data": history
        }

    def create_integration(self, integration_name, integration_type, target_schema, source_schema, spark_config,
                           hadoop_config, cron_expression, source_connection,target_connection, source_table, target_table,
                           batch_size):
        scheduler = OpenETLIntegrations(
            integration_name=integration_name,
            integration_type=integration_type,
            cron_expression=cron_expression,
            source_connection=source_connection,
            target_connection=target_connection,
            source_table=source_table,
            target_table=target_table,
            spark_config=spark_config,
            hadoop_config=hadoop_config,
            source_schema=source_schema,
            target_schema=target_schema,
            batch_size=batch_size
        )

        self.session.add(scheduler)
        self.session.commit()
        return scheduler

    def delete_integration(self, record_id):
        self.session.query(OpenETLIntegrations).filter(OpenETLIntegrations.id == record_id).delete(synchronize_session=False)
        self.session.commit()

    def update_integration(self, record_id, **kwargs):
        batch = self.session.query(OpenETLIntegrations).filter(OpenETLIntegrations.id == record_id).first()

        if not batch:
            return NoResultFound
        for key, value in kwargs.items():
            if hasattr(batch, key):
                setattr(batch, key, value)
        self.session.commit()
        return batch

    def update_integration_runtime(self, job_id, **kwargs):
        batch = self.session.query(OpenETLIntegrationsRuntimes).filter(
            OpenETLIntegrationsRuntimes.integration == job_id,
            OpenETLIntegrationsRuntimes.celery_task_id == job_id
        ).order_by(OpenETLIntegrationsRuntimes.created_at.desc()).first()

        if not batch:
            raise NoResultFound

        for key, value in kwargs.items():
            if hasattr(batch, key):
                if key == 'row_count_trg':
                    # Add the row_count_trg value if it already exists in the database record
                    current_row_count = getattr(batch, 'row_count_trg', 0)
                    if current_row_count is None:
                        current_row_count = 0
                    setattr(batch, 'row_count_trg', current_row_count + value)
                else:
                    setattr(batch, key, value)

        self.session.commit()
        return batch

    def get_integrations_to_schedule(self) -> list[Type[OpenETLIntegrations]]:
        return self.session.query(OpenETLIntegrations).all()


    def create_integration_history(self, **kwargs):
        scheduler = OpenETLIntegrationsRuntimes(**kwargs)
        self.session.add(scheduler)
        self.session.commit()
        return scheduler


def get_open_etl_document_connection_details(url=False):
    """Get connection details for OpenETL Document"""

    if url:
        return "postgresql+psycopg2://{username}:{password}@{hostname}:{port}/{database}".format(
            username=os.getenv("OPENETL_DOCUMENT_USER","rusab1"),
            password=os.getenv("OPENETL_DOCUMENT_PASS","1234"),
            hostname=os.getenv("OPENETL_DOCUMENT_HOST","localhost"),
            port=os.getenv("OPENETL_DOCUMENT_PORT","5432"),
            database=os.getenv("OPENETL_DOCUMENT_DB","airflow")
        )
    return {
        "engine": os.getenv("OPENETL_DOCUMENT_ENGINE","PostgreSQL"),
        "hostname": os.getenv("OPENETL_DOCUMENT_HOST","localhost"),
        "username": os.getenv("OPENETL_DOCUMENT_USER","rusab1"),
        "password": os.getenv("OPENETL_DOCUMENT_PASS","1234"),
        "port": os.getenv("OPENETL_DOCUMENT_PORT","5432"),
        "database": os.getenv("OPENETL_DOCUMENT_DB","airflow")
    }

def generate_cron_expression(schedule_time, schedule_dates=None, frequency=None):
    """
    Generates a cron expression based on provided scheduling details.

    :param schedule_time: Time string in 'HH:MM:SS' format.
    :param schedule_dates: List of date strings in 'YYYY-MM-DD' format or None.
    :param frequency: A string defining the frequency ('daily', 'weekly', 'hourly') or None.
    :return: List of cron expression strings.
    """
    time_parts = schedule_time.split(':')
    minute = time_parts[1]
    hour = time_parts[0]

    if frequency:
        frequency = frequency.lower()

        if frequency == 'hourly':
            return [f"0 * * * *"]
        elif frequency == 'daily':
            return [f"{minute} {hour} * * *"]
        elif frequency == 'weekly':
            return [f"{minute} {hour} * * 0"]
        else:
            raise ValueError("Unsupported frequency value. Use 'hourly', 'daily', or 'weekly'.")

    if schedule_dates:
        cron_expressions = []
        for date_str in schedule_dates:
            date_obj = datetime.strptime(date_str, "%Y-%m-%d")
            day = date_obj.day
            month = date_obj.month
            cron_expressions.append(f"{minute} {hour} {day} {month} *")
        return cron_expressions

    raise ValueError("Either frequency or schedule_dates must be provided.")



def parse_cron_expression(cron_expr):
    """
    Parses a cron expression into its components, determines the next execution time,
    and provides a human-readable explanation.

    Args:
    - cron_expr (str): A standard 5-part cron expression.

    Returns:
    - dict: Parsed cron details and next execution time.
    """
    cron_parts = cron_expr.split()
    if len(cron_parts) != 5:
        raise ValueError("Invalid cron expression. It should have exactly 5 parts.")

    minute, hour, day_of_month, month, day_of_week = cron_parts

    # Component formatting function
    def format_component(component, name):
        if component == "*":
            return f"Every {name}"
        elif "," in component:
            return f"Multiple {name}s: {component}"
        elif "-" in component:
            return f"Range of {name}s: {component}"
        elif "*/" in component:
            return f"Every {component[2:]} {name}s"
        return f"Specific {name}: {component}"

    # Convert cron values to detailed text
    details = {
        "minute": format_component(minute, "minute"),
        "hour": format_component(hour, "hour"),
        "day_of_month": format_component(day_of_month, "day of month"),
        "month": format_component(month, "month"),
        "day_of_week": format_component(day_of_week, "day of week"),
    }

    # Compute next execution using croniter (prevents infinite loops)
    now = datetime.now()
    cron = croniter(cron_expr, now)
    next_execution = cron.get_next(datetime)

    next_execution_12hr = next_execution.strftime('%I:%M %p')

    # Explanation
    explanation = (
        f"This schedule runs at {details['hour']}:{details['minute']} on {details['day_of_month']} "
        f"during {details['month']}. It happens on {details['day_of_week']}. "
        f"The next execution will be at {next_execution_12hr} on {next_execution.strftime('%B %d, %Y')}."
    )

    return {
        **details,
        "next_execution": next_execution_12hr,
        "next_execution_full": next_execution.strftime('%Y-%m-%d %I:%M %p'),
        "cron_expression": cron_expr,
        "explanation": explanation
    }
