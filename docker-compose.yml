version: '3.9'

services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
      args:
        - DOCKER_BUILDKIT=1
    container_name: openetl-backend
    networks:
      - openetl-network
    ports:
      - "5009:5009"
    env_file:
      - .env
    restart: always

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
    container_name: openetl-frontend
    networks:
      - openetl-network
    ports:
      - "8500:8500"
    environment:
      - BACKEND_HOST=http://openetl-backend:5009
      - OPENETL_HOME=/app
    depends_on:
      - backend
    restart: always

  spark-master:
    image: bitnami/spark:3.5.3
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    ports:
      - "8080:8080"  # Spark Web UI
      - "7077:7077"  # Spark Master
    networks:
      - openetl-network


    restart: always

  spark-worker-1:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks:
      - openetl-network
    restart: always



  spark-worker-2:
    image: bitnami/spark:3.5.3
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
    networks:
      - openetl-network
    restart: always

  redis:
    image: redis
    container_name: redis
    networks:
      - openetl-network
    ports:
      - "6379:6379"
    restart: always

  scheduler:
    container_name: openetl-scheduler
    image: openetl-backend  # Use the backend image
    command: python3 utils/scheduler_utils.py
    env_file:
      - .env
    networks:
      - openetl-network
    restart: always

  celery_worker:
    container_name: openetl-celery-worker
    image: openetl-backend  # Use the backend image
    command: celery -A utils.celery_utils worker --loglevel=info
    networks:
      - openetl-network
    env_file:
      - .env
    depends_on:
      - redis
    restart: always

networks:
  openetl-network:
    driver: bridge
